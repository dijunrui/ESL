## 1. Introduction

__The performance of a learning method relates to its prediction capability on independent test data__

## 2. Bias, Variance, and MOdel Complexity

### 2.1 Regression

* __Test error__: aka _generalization error_, is the prediction error over an independent test sample $\text{Err}_\tau = E{L(Y, \hat{f}(x))|\tau}$. $\tau$ represents a fixed training set.

* __Expected prediction error__: or _expected test error_ $\text{Err} = E[L(Y,\hat{f}(X))] = E[\text{Err}_{\tau}]$. Expectation averages over everything that is random, including the randomness in the training set that produced $\hat{f}$. 

* __Training error__: is the average loss over the training sample $\bar{\text{err}}= \frac{1}{N}\sum_{i=1}^N L(y_i, \hat{f}(x_i))$


### 2.2 Classification
 Loss function can be 0-1 loss oir $-2\times$log likelihood (aka deviance.)
 

### 2.3 Split of data

Training (fit the model), validation (model selection), and test (model assessment). 

## 3. Bia-Variance Decomposition

The expected prediction error of a regression fit $\hat{f}(x)$ at input $X = x_0$
$$\begin{aligned}
\text{Err}(x_0) &= E[(Y - \hat{f}(x_0))^2|X=x_0]\\
&= \sigma^2_\epsilon + [E\hat{f}(x_0) - f(x_0)]^2 + E[\hat{f}(x_0) - E\hat{f}(x_0)]^2\\
&= \sigma^2_\epsilon + \text{Bias}^2(\hat{f}(x_0)) + \text{Var}(\hat{f}(x_0))
\end{aligned}$$

## 4. The Optimism of the Training Error Rate

## 5. Estimates of In-Sample Prediction Error

$C_p = \bar{\text{err}} + 2 \frac{d}{N}\hat{\sigma}^2_\epsilon$, $d$ is the number of inputs or basis functions

$AIC = -\frac{2}{N}\text{loglik} + 2\frac{d}{N}$

$AIC(\alpha) = \bar{\text{err}}(\alpha) + 2 \frac{d(\alpha)}{N}\hat{\sigma}^2_\epsilon$ for models with tuning parameter $\alpha$

## 6. Effective Number of Parameters

## 7. The Bayesian Approachj and BIC

General form $\text{BIC} = -2\text{loglik} + (\log N)d$

## 10. Cross Validation

Goal: To estimate prediction error. It estimates the expected extra-sample error $\text{Err} = E[L(Y,\hat{f}(X))]$

### 10.1 K Fold CV

Split the data into $K$ parts with equal size. For the $k$th part, we fit the model to the other $K-1$ parts of the data and calculate the prediction error of the fitted model when predicting the $k$th part of the data. We do this for $k = 1,2,...K$ and combine the $K$ estimates of the prediction errors. 

$\mathcal{k}:\{1...N\} \rightarrow \{1...K\}$ represents the partition. $\hat{f}^{-k}(x)$ denoted the fitted value while excluding the set $k$. The cross validation estimate of the prediction error is 

$$\text{CV}(\hat{f}) = \frac{1}{N}\sum_{i=1}^NL(y_i - \hat{f}^{-k}(x))$$

or 
$$\text{CV}(\hat{f}) = \frac{1}{K}\sum_{k=1}^K [\frac{1}{n_k}\sum_{i \in \text{kth val set}}L(y_i - \hat{f}^{-k}(x))]$$

LOCV: Low bias but high variance, also high computational burden.  

__One Standard Error Rule__ Choose the most parsimonious model whose error is no more than one standard error above the error of the best model. 

__Generalized Cross Validation__ A convenient approximation to leave one out CV, for linear fitting under squared error loss. 

$$GCV(\hat{f}) = \frac{1}{N}\sum_i [\frac{y_i - \hat{f}(x_i)}{1-\text{trace}(\mathbf{S})/N}]^2$$
### 10.2 Tips for CV

DON'T subset the data before CV, because the subset was chosen on the basis of all of the samples. Leaving samples out after the subset does not correctly mimic the application of the classifier. 

In general, with a multistep modeling procedure, cross validation must be applied to the entire sequence of modeling steps. 


## 11. Bootstrap





